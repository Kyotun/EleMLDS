% Teaching content definitions.
% Use \DefineTeach{<number>}{...} where <number> matches the section numbering.
% Examples: 1, 1.2, 1.2.1

% 1) INTRODUCTION TO DATA SCIENCE
% 1.2 Tabular Data
%
% Use the structure below when adding teaching content.
% Note: \DefineTeach{1.2} is just an example to illustrate the format.
% \DefineTeach{1.2}{%
% % Write teaching content for "1.2 Tabular Data" here.
%
% % Template (copy and adapt):
% % \begin{examlikelihood}{Medium}
% % Why this appears on exams and how to recognize it.
% % \end{examlikelihood}
% %
% % \begin{examfavorite}
% % What instructors love to ask and typical phrasing.
% % \end{examfavorite}
% %
% % Why (motivation): ...
% % What (definition): ...
% % How (procedure/usage): ...
% % Advantages: ...\\
% % Limitations: ...
% %
% % \begin{cheatsheet}
% % \begin{itemize}
% %   \item Must‑memorize point 1
% %   \item Must‑memorize point 2
% % \end{itemize}
% % \end{cheatsheet}
% %
% % \begin{pitfall}
% % Common mistake and how to avoid it.
% % \end{pitfall}
% %
% % \begin{visualbox}
% % \centering
% % \begin{tikzpicture}[]
% % % Simple diagram
% % \end{tikzpicture}
% % \end{visualbox}
% %
% % \textbf{Key takeaways:} ...
% % }

% 1.4 Data Types
\DefineTeach{1.4}{%
\begin{visualbox}
\centering
\includegraphics[width=0.9\linewidth]{asset/data-types.png}
\end{visualbox}

Advantages: clarifies variable handling and measurement choices.\\
Limitations: real data can mix types or sit between categories.
}

% 1.5 Descriptive Statistics
\DefineTeach{1.5}{%
\begin{examlikelihood}{High}
Frequent: compute variance/STD/covariance/correlation by hand; read a correlation matrix.
Answer: apply sample formulas, compute values, and interpret sign/magnitude; matrix is symmetric with 1s on the diagonal.
\end{examlikelihood}

\begin{examfavorite}
Explain why covariance depends on units, and why correlation is normalized in $[-1,1]$.
Answer: covariance scales with units; correlation divides by SDs so it is unitless and bounded.
\end{examfavorite}

Why (motivation): Quantify spread and association between variables.\\
What (definition): Variance/STD measure spread; covariance/correlation measure linear association.\\
How (procedure/usage): Compute formulas, then interpret sign/magnitude and check the correlation matrix.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Variance (sample):} $s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar{x})^2$
  \item \textbf{Std dev:} $s = \sqrt{s^2}$
  \item \textbf{Covariance (sample):} $\mathrm{cov}(X,Y)=\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})$
  \item \textbf{Correlation:} $r=\frac{\mathrm{cov}(X,Y)}{s_X s_Y}$ \; (unitless, $-1$ to $1$)
  \item \textbf{Correlation matrix:} table of pairwise correlations; symmetric with 1s on the diagonal.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Correlation $\neq$ causation; a strong correlation can be driven by a confounder or Simpson's paradox.
\end{pitfall}

Advantages: quick, compact summaries of spread and association.\\
Limitations: can hide distribution shape and outliers.

\begin{visualbox}
\centering
\begin{tikzpicture}[
  label/.style={font=\scriptsize},
]
% 3x3 correlation matrix sketch
\draw[thick] (0,0) rectangle (1.8,1.8);
\draw (0.6,0) -- (0.6,1.8);
\draw (1.2,0) -- (1.2,1.8);
\draw (0,0.6) -- (1.8,0.6);
\draw (0,1.2) -- (1.8,1.2);
\node[label] at (0.3,1.5) {1};
\node[label] at (0.9,1.5) {$r_{12}$};
\node[label] at (1.5,1.5) {$r_{13}$};
\node[label] at (0.3,0.9) {$r_{21}$};
\node[label] at (0.9,0.9) {1};
\node[label] at (1.5,0.9) {$r_{23}$};
\node[label] at (0.3,0.3) {$r_{31}$};
\node[label] at (0.9,0.3) {$r_{32}$};
\node[label] at (1.5,0.3) {1};
\node[label, right] at (2.1,0.9) {Correlation matrix};
\end{tikzpicture}
\end{visualbox}

\textbf{Key takeaways:} Know formulas + interpretations; correlation matrix is symmetric with 1s on the diagonal.
}

% 1.6 Basic Visualizations
\DefineTeach{1.6}{%
\begin{visualbox}
\centering
\includegraphics[width=0.9\linewidth]{asset/box-plot.png}
\end{visualbox}

Advantages: makes distribution and outliers visible at a glance.\\
Limitations: can hide multimodality or sample size differences.
}

% 1.7 Feature Transformations
\DefineTeach{1.7}{%
\begin{examlikelihood}{High}
Typical: pick the right transform (scale, log, encode) and explain why.
Answer: choose encoding by category type and scaling/log for skew; justify the choice.
\end{examlikelihood}

\begin{examfavorite}
Identify data leakage in preprocessing; name the correct order for train/test transformations.
Answer: fit transforms on training data only, then apply to validation/test.
\end{examfavorite}

Why (motivation): Turn raw categorical/continuous variables into model-ready features.\\
What (definition): Encoding or discretizing features without changing the target meaning.\\
How (procedure/usage): Choose encoding by category type; choose binning by distribution.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{One-hot encoding:} create a 0/1 column per category (nominal).
  \item \textbf{Binary encoding:} represent categories as binary digits (compact one-hot).
  \item \textbf{Ordinal encoding:} map ordered categories to ranks (only if order is real).
  \item \textbf{Binning:} convert continuous to categories.
  \item \textbf{Equal-width binning:} fixed interval sizes across the range.
  \item \textbf{Equal-frequency binning:} same number of samples per bin.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Fitting transforms on the full dataset (leakage). Always fit on training data, then apply to validation/test.
\end{pitfall}

Advantages: improves model performance and stability.\\
Limitations: can reduce interpretability or introduce leakage if misapplied.

\textbf{Key takeaways:} Use one-hot for nominal, ordinal for ordered labels, and binning for simplification.
}

% 2.1 Introduction to Decision Trees
\DefineTeach{2.1}{%
\begin{examlikelihood}{High}
Intro questions often ask you to explain how trees split data and what leaves represent.
Answer: describe root/internal/leaf roles and how splits partition the feature space.
\end{examlikelihood}

\begin{examfavorite}
Draw a small tree from a toy dataset or explain interpretability vs overfitting.
Answer: build simple if-then splits and note that deeper trees overfit without pruning.
\end{examfavorite}

Why (motivation): Learn a function from labeled training instances to make predictions.\\
What (definition): A tree partitions the feature space by sequential if-then splits; leaves output a class or value.\\
How (procedure/usage): Choose splits to improve class purity or reduce prediction error.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Goal:} learn a function $f(X)$ from labeled data to predict labels/values.
  \item \textbf{Tree structure:} root node, internal (non-leaf) nodes, leaf nodes.
  \item \textbf{Split rule:} one feature + threshold; paths are if-then rules.
  \item \textbf{Leaf meaning:} prediction (class/value) for that region of the space.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Overly deep trees memorize training data; control with max depth, min samples, or pruning.
\end{pitfall}

Advantages: interpretable rules and fast inference.\\
Limitations: high variance and prone to overfitting without pruning.

\begin{visualbox}
\centering
\begin{tikzpicture}[
  node distance=8mm and 12mm,
  box/.style={draw, rounded corners, align=center, minimum width=26mm, minimum height=8mm},
  arrow/.style={-Latex, thick}
]
\node[box] (root) {$x_1 < 5?$};
\node[box, below left=of root] (l) {Leaf: Class A};
\node[box, below right=of root] (r) {$x_2 < 3?$};
\node[box, below left=of r] (rl) {Leaf: Class B};
\node[box, below right=of r] (rr) {Leaf: Class A};
\draw[arrow] (root) -- (l);
\draw[arrow] (root) -- (r);
\draw[arrow] (r) -- (rl);
\draw[arrow] (r) -- (rr);
\end{tikzpicture}
\end{visualbox}

\textbf{Key takeaways:} Trees learn $f$ from labeled data using splits; nodes/leaf roles are core.
}

% 2.2 Entropy and Information Gain
\DefineTeach{2.2}{%
\begin{examlikelihood}{Very High}
Almost guaranteed: compute entropy and information gain for candidate splits.
Answer: compute parent entropy, weighted child entropies, then IG.
\end{examlikelihood}

\begin{examfavorite}
Given a small labeled dataset, compare splits and pick the one with highest information gain.
Answer: calculate IG for each split and select the largest.
\end{examfavorite}

Why (motivation): Choose splits that make child nodes as pure as possible.\\
What (definition): Entropy measures impurity; information gain is impurity reduction.\\
How (procedure/usage): Compute parent entropy, child entropies, then IG = parent - weighted children.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Entropy:} $H(S)=-\sum_{c} p_c \log_2 p_c$ (define $0\log 0 = 0$).
  \item \textbf{Weighted child entropy:} $\sum_{k}\frac{|S_k|}{|S|} H(S_k)$.
  \item \textbf{Information gain:} $\mathrm{IG}(S, \text{split})=H(S)-\sum_{k}\frac{|S_k|}{|S|} H(S_k)$.
  \item \textbf{Goal:} choose split with highest IG (most impurity reduction).
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Forgetting to weight child entropies by subset size; using raw entropy sums gives wrong IG.
\end{pitfall}

Advantages: principled split criterion that increases purity.\\
Limitations: IG is biased toward many-valued attributes.

\begin{visualbox}
\centering
\includegraphics[width=0.75\linewidth]{asset/entropy-formula.png}
\vspace{2mm}
\includegraphics[width=0.75\linewidth]{asset/overall-entropy.png}
\vspace{2mm}
\includegraphics[width=0.75\linewidth]{asset/information-gain.png}
\end{visualbox}

\textbf{Key takeaways:} Compute entropy, weight children, pick split with highest IG.
}

% 2.3 ID3 Algorithm
\DefineTeach{2.3}{%
\begin{examlikelihood}{Very High}
Common: list the ID3 steps or run one iteration to choose the best split.
Answer: check stopping, compute IG per attribute, split on max IG, recurse.
\end{examlikelihood}

\begin{examfavorite}
Explain stopping conditions and why ID3 prefers high information gain.
Answer: stop if pure/no attributes; IG maximizes impurity reduction.
\end{examfavorite}

Why (motivation): Build a decision tree that best separates labeled data.\\
What (definition): ID3 is a greedy, top-down tree induction algorithm using information gain.\\
How (procedure/usage): Compute IG for each attribute, split on the best, and recurse.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Input:} labeled dataset $S$ with categorical attributes (ID3 original).
  \item \textbf{Step 1:} if all labels same $\rightarrow$ make a leaf.
  \item \textbf{Step 2:} if no attributes left $\rightarrow$ leaf with majority class.
  \item \textbf{Step 3:} choose attribute with highest IG.
  \item \textbf{Step 4:} split $S$ by attribute values and recurse.
  \item \textbf{Output:} a decision tree; leaves store class label.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
ID3 favors attributes with many values; without corrections (e.g., gain ratio) it can overfit.
\end{pitfall}

Advantages: simple, fast, and easy to explain.\\
Limitations: greedy (not optimal), biased to many-valued attributes.

\begin{visualbox}
\centering
\includegraphics[width=0.75\linewidth]{asset/id3-algo.png}
\vspace{2mm}
\includegraphics[width=0.75\linewidth]{asset/id3-example1.png}
\vspace{2mm}
\includegraphics[width=0.75\linewidth]{asset/id3-example2.png}
\end{visualbox}

\textbf{Key takeaways:} ID3 is greedy; compute IG, split, recurse, stop with pure/majority leaves.
}

% 2.4 Quantifying Information Gain
\DefineTeach{2.4}{%
\begin{examlikelihood}{Very High}
Often compute IG for a specific split and compare candidate attributes.
Answer: show parent entropy, weighted children, and IG (optionally gain ratio).
\end{examlikelihood}

\begin{examfavorite}
Show all intermediate steps: parent entropy, each child entropy, weighted sum, IG.
Answer: compute each step explicitly and report the final IG (and GR if asked).
\end{examfavorite}

Why (motivation): Convert “best split” into a concrete, comparable number.\\
What (definition): IG = parent entropy minus weighted child entropies; Split Info measures how evenly the split divides data; Gain Ratio normalizes IG.\\
How (procedure/usage): Compute IG, then divide by split info to get gain ratio.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Step 1:} compute parent entropy $H(S)$.
  \item \textbf{Step 2:} split by attribute values.
  \item \textbf{Step 3:} compute each child entropy $H(S_k)$.
  \item \textbf{Step 4:} compute weighted sum $\sum_k \frac{|S_k|}{|S|}H(S_k)$.
  \item \textbf{Step 5:} IG $= H(S) - \sum_k \frac{|S_k|}{|S|}H(S_k)$.
  \item \textbf{Split info (lecture: $H(d)$):} entropy of split proportions (how evenly data is partitioned).\\
  \hspace*{2mm}$H(d)=SI = -\sum_k \frac{|S_k|}{|S|}\log_2 \frac{|S_k|}{|S|}$.
  \item \textbf{Gain ratio:} $GR = \frac{IG}{H(d)}$ (same as $IG/SI$; penalizes many-valued attributes).
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Information gain is biased toward attributes with many values; use gain ratio to correct.
Also: weight by subset size and keep log base consistent.
\end{pitfall}

Advantages: makes split comparisons explicit and quantitative.\\
Limitations: IG can favor attributes with many values without normalization.

\begin{visualbox}
\centering
\includegraphics[width=0.75\linewidth]{asset/ig2-1.png}
\vspace{2mm}
\includegraphics[width=0.75\linewidth]{asset/igr-2.png}
\end{visualbox}

\textbf{Key takeaways:} IG is a weighted impurity reduction; higher is better.
}

% 2.5 Pruning
\DefineTeach{2.5}{%
\begin{examlikelihood}{High}
Often: explain why pruning reduces overfitting and name pre- vs post-pruning.
Answer: pruning removes low-value branches to reduce variance; name pre/post pruning.
\end{examlikelihood}

\begin{examfavorite}
Given a tree, identify which branches to prune using validation error or complexity.
Answer: prune branches that do not improve validation performance or reduce cost-complexity.
\end{examfavorite}

Why (motivation): Reduce overfitting by simplifying a deep tree.\\
What (definition): Pruning removes splits/branches that do not improve generalization.\\
How (procedure/usage): Stop early (pre-pruning) or cut back after training (post-pruning).

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Pre-pruning:} stop splitting early using rules like:
  max depth, min samples per node, min impurity decrease, min samples per leaf.
  \item \textbf{Post-pruning:} grow full tree, then prune using validation error or cost-complexity.
  \item \textbf{Goal:} simpler tree with similar or better validation performance.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Pruning too aggressively can underfit; always tune on validation data, not test data.
\end{pitfall}

Advantages: improves generalization by reducing variance.\\
Limitations: too much pruning increases bias.

\begin{visualbox}
\centering
\includegraphics[width=0.75\linewidth]{asset/post-pruning.png}
\end{visualbox}

\textbf{Key takeaways:} Pruning trades depth for generalization; use validation to choose.
}

% 2.6 Continuous Data (Threshold splits)
\DefineTeach{2.6}{%
\begin{examlikelihood}{High}
Common: show how a continuous feature is split using a threshold and compute the best split score.
Answer: sort values, test midpoints, and choose the split with highest IG (or variance reduction).
\end{examlikelihood}

\begin{examfavorite}
Given sorted values, test candidate thresholds and pick the one with maximum information gain.
Answer: evaluate IG at valid midpoints and choose the maximum.
\end{examfavorite}

Why (motivation): Many real-world features are continuous; trees need a simple yes/no split.\\
What (definition): A threshold split picks a value $t$ so that $x_j \le t$ goes left and $x_j > t$ goes right.\\
How (procedure/usage): Sort values, test candidate thresholds (midpoints between distinct neighbors), compute split score, choose best, recurse.

Variance in a node/leaf (regression trees): For a node with targets $y_1,\dots,y_n$ and mean $\bar{y}$,
the node variance (impurity) is the average squared deviation from the mean; a good split minimizes the
weighted variance of the child nodes (equivalently, maximizes variance reduction).

\textbf{Mini example (regression):} Data $(x,y)$ = $(1,2),(2,2),(3,3),(4,10),(5,11),(6,12)$.\\
Parent variance $\approx 19.22$. Try two thresholds:
\begin{itemize}
  \item $t=3.5$: left $y=\{2,2,3\}$ var $\approx 0.22$, right $y=\{10,11,12\}$ var $\approx 0.67$.
  Weighted variance $=(3/6)\cdot 0.22 + (3/6)\cdot 0.67 \approx 0.45$.
  \item $t=1.5$: left $y=\{2\}$ var $=0$, right $y=\{2,3,10,11,12\}$ var $\approx 17.84$.
  Weighted variance $\approx (5/6)\cdot 17.84 = 14.87$.
\end{itemize}
So $t=3.5$ is preferred because it gives much lower weighted variance.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Split rule:} $x_j \le t$ vs $x_j > t$ (binary split).
  \item \textbf{Candidates:} midpoints between sorted unique values; often only where class label changes.
  \item \textbf{Score (classification):} maximize information gain (IG).
  \item \textbf{Variance in a node:} $\frac{1}{n}\sum_{i=1}^{n}(y_i-\bar{y})^2$ (SSE/variance impurity).
  \item \textbf{Score (regression):} minimize weighted child variance / MSE (maximize variance reduction).
  \item \textbf{Efficiency:} sort once ($O(n\log n)$), then sweep thresholds.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Using thresholds that create empty or nearly empty children (e.g., duplicates without midpoints); always use distinct values and check child sizes.
\end{pitfall}

Advantages: enables trees to handle continuous features.\\
Limitations: many candidate splits; sensitive to noise/outliers.

\begin{visualbox}
\centering
\includegraphics[width=0.78\linewidth]{asset/continuous-decision1.png}
\vspace{2mm}
\includegraphics[width=0.78\linewidth]{asset/continuous-decision2.png}
\vspace{2mm}
\includegraphics[width=0.78\linewidth]{asset/continuous-decision3.png}
\end{visualbox}

\textbf{Key takeaways:} Continuous features are split by thresholds; evaluate candidate midpoints and pick the best impurity reduction.
}

% 2.7 Ensembles (Bagging/Random Forest/Boosting)
\DefineTeach{2.7}{%
\begin{examlikelihood}{High}
Short theory questions about why ensembles help and how they are built are common.
Answer: averaging many trees reduces variance and stabilizes predictions.
\end{examlikelihood}

\begin{examfavorite}
Explain bias--variance intuition: averaging many trees reduces variance and improves stability.
Answer: averaging cancels noise, lowering variance and improving generalization.
\end{examfavorite}

Why (motivation): Single trees are high-variance; ensembles stabilize predictions and improve accuracy.\\
What (definition): An ensemble combines many base models (often trees) into one predictor.\\
How (procedure/usage): Train multiple trees (e.g., on resampled data) and aggregate their outputs by vote or average.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Goal:} reduce variance and improve generalization.
  \item \textbf{Combine:} predictions are averaged (regression) or majority-voted (classification).
  \item \textbf{Intuition:} many weak/unstable trees $\rightarrow$ one strong, stable predictor.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Thinking more trees always fix bias; ensembles mainly reduce variance, not poor features or labels.
\end{pitfall}

Advantages: strong accuracy and stability.\\
Limitations: reduced interpretability and higher compute.

\begin{visualbox}
\centering
\begin{tikzpicture}[
  box/.style={draw, rounded corners, align=center, minimum width=20mm, minimum height=6mm},
  arrow/.style={-Latex, thick},
  node distance=6mm and 8mm
]
\node[box] (data) {Data};
\node[box, right=of data] (t1) {Tree 1};
\node[box, above=of t1] (t2) {Tree 2};
\node[box, below=of t1] (t3) {Tree 3};
\node[box, right=of t1] (agg) {Vote/Avg};
\draw[arrow] (data) -- (t1);
\draw[arrow] (data) -- (t2);
\draw[arrow] (data) -- (t3);
\draw[arrow] (t1) -- (agg);
\draw[arrow] (t2) -- (agg);
\draw[arrow] (t3) -- (agg);
\end{tikzpicture}
\end{visualbox}

\textbf{Key takeaways:} Ensembles combine many trees to reduce variance and stabilize predictions.
}

% 3.1 Introduction to Unsupervised Learning
\DefineTeach{3.1}{%
\begin{examlikelihood}{High}
Often asked: define unsupervised learning and contrast it with supervised learning.
Answer: unsupervised has no labels; it discovers structure like clusters.
\end{examlikelihood}

\begin{examfavorite}
Explain what “no labels” means and give a concrete task like clustering or dimensionality reduction.
Answer: only $X$ is given; tasks include clustering and dimensionality reduction.
\end{examfavorite}

Why (motivation): Labels are expensive or unavailable; we still want structure in the data.\\
What (definition): Unsupervised learning finds patterns, groups, or low‑dimensional structure without target labels.\\
How (procedure/usage): Choose a goal (grouping, structure, anomaly detection), define a similarity measure, then apply an algorithm.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{No labels:} only $X$ (features), no $y$.
  \item \textbf{Main tasks:} clustering, dimensionality reduction, anomaly detection.
  \item \textbf{Key idea:} “similar” points should end up close or in the same group.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Interpreting clusters as ground truth classes without validation; clusters depend on distance metrics and scaling.
\end{pitfall}

Advantages: finds structure without labels.\\
Limitations: results can be subjective and metric-dependent.

\textbf{Key takeaways:} Unsupervised learning discovers structure without labels; clustering groups similar points.
}

% 3.2 Introduction to Clustering
\DefineTeach{3.2}{%
\begin{examlikelihood}{High}
Expect definitions and a short compare/contrast of clustering goals or algorithms.
Answer: clustering groups similar points; results depend on metric and method.
\end{examlikelihood}

\begin{examfavorite}
Explain what a cluster is and why distance/similarity choice matters.
Answer: clusters maximize within-group similarity; metric choice changes the grouping.
\end{examfavorite}

Why (motivation): We want to group similar observations when no labels exist.\\
What (definition): Clustering partitions data into groups so points in the same cluster are more similar to each other than to points in other clusters.\\
How (procedure/usage): Choose a similarity/distance metric, pick a clustering method, then evaluate/interpret the groups.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Goal:} high within‑cluster similarity, low between‑cluster similarity.
  \item \textbf{Distance matters:} Euclidean, Manhattan, cosine, etc., change the result.
  \item \textbf{Outputs:} cluster labels (hard) or membership scores (soft).
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Clustering is not “ground truth”; different scaling or distance metrics can change clusters drastically.
\end{pitfall}

Advantages: reveals hidden group structure.\\
Limitations: sensitive to metric, scale, and algorithm choice.

\textbf{Key takeaways:} Clustering groups similar points; metric choice and scaling drive the result.
}

% 3.3 Similarity and Dissimilarity
\DefineTeach{3.3}{%
\begin{examlikelihood}{High}
Common: compute a distance/similarity and explain when to use each metric.
Answer: apply the correct formula and justify the metric for the data type.
\end{examlikelihood}

\begin{examfavorite}
Compare Euclidean vs Manhattan vs cosine; note the role of scaling/normalization.
Answer: Euclidean (L2) vs Manhattan (L1) for continuous data, cosine for direction; scale features first.
\end{examfavorite}

Why (motivation): Clustering depends on “closeness”; wrong metric gives wrong groups.\\
What (definition): Similarity measures how alike points are; dissimilarity (distance) measures how far apart they are.\\
How (procedure/usage): Choose a metric that matches data type and scale; normalize features when needed.

\textbf{Goal statement:} Maximize similarity within the same group and maximize dissimilarity between different groups.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Jaccard (nominal/binary):} $J(A,B)=\frac{|A\cap B|}{|A\cup B|}$; distance $d_J=1-J$.
  \item \textbf{Minkowski (continuous):} $d_p(x,y)=\left(\sum_i |x_i-y_i|^p\right)^{1/p}$.
  \item \textbf{Manhattan:} $d_1=\sum_i |x_i-y_i|$ (Minkowski with $p=1$).
  \item \textbf{Euclidean:} $d_2=\sqrt{\sum_i (x_i-y_i)^2}$ (Minkowski with $p=2$).
  \item \textbf{Chebyshev:} $d_\infty=\max_i |x_i-y_i|$ (Minkowski as $p\to\infty$).
  \item \textbf{Scaling:} standardize/normalize when units differ.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Using Euclidean on unscaled features (one large‑unit feature dominates).
\end{pitfall}

Advantages: lets you tailor similarity to data type.\\
Limitations: wrong metric or scaling yields misleading clusters.

\textbf{Which to use (rule of thumb):}
\begin{itemize}
  \item \textbf{Jaccard:} nominal/binary attributes or set‑like data (presence/absence).
  \item \textbf{Manhattan:} high‑dimensional data or when you want axis‑aligned distance.
  \item \textbf{Euclidean:} continuous features with comparable scale and spherical clusters.
  \item \textbf{Chebyshev:} when the maximum coordinate difference is what matters (strict tolerance).
\end{itemize}

\begin{visualbox}
\centering
\includegraphics[width=0.8\linewidth]{asset/similarity.png}
\vspace{2mm}
\includegraphics[width=0.8\linewidth]{asset/similarity2.png}
\end{visualbox}

\textbf{Key takeaways:} Metric choice and scaling control the notion of similarity and the resulting clusters.
}

% 3.4 K-means and K-medoids
\DefineTeach{3.4}{%
\begin{examlikelihood}{High}
Often: list K-means steps and compare K-means vs K-medoids.
Answer: K-means = init/assign/update/repeat; K-medoids uses medoids and is more robust.
\end{examlikelihood}

\begin{examfavorite}
Run 1 iteration of K-means by hand (assign \& update) or explain why K-medoids is more robust to outliers.
Answer: compute distances, assign clusters, recompute means; medoids resist outliers.
\end{examfavorite}

Why (motivation): Need a simple, scalable way to cluster continuous data.\\
What (definition): K-means uses centroids (means) to minimize within‑cluster SSE; K-medoids uses actual data points as centers and minimizes total within‑cluster distance.\\
How (procedure/usage): Initialize $k$ centers, assign each point to nearest center, update centers, repeat until convergence.

\textbf{Comparison:} K-means is faster but sensitive to outliers and scaling; K-medoids is more robust but slower.\\
\textbf{Difference:} K-means centers are means (can be non‑data points); K-medoids centers are actual data points.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{K-means objective:} minimize $\sum_{k=1}^{K}\sum_{x_i\in C_k}\|x_i-\mu_k\|^2$.
  \item \textbf{K-means steps:} (1) choose $K$ and initialize $\mu_k$;
  (2) assign each point to nearest $\mu_k$;
  (3) update $\mu_k$ to the mean of its cluster;
  (4) repeat 2–3 until assignments stop changing.
  \item \textbf{K-means optimization:} alternating minimization (assignment step + centroid update); decreases SSE each iteration.
  \item \textbf{K-medoids center:} medoid $m_k$ is a data point minimizing total distance in its cluster.
  \item \textbf{K-medoids steps:} (1) choose $K$ medoids $m_k$;
  (2) assign each point to nearest medoid;
  (3) for each cluster, swap medoid candidate to reduce total distance;
  (4) repeat 2–3 until no improvement.
  \item \textbf{K-medoids optimization:} minimize $\sum_{k}\sum_{x_i\in C_k} d(x_i,m_k)$ via swap heuristics (e.g., PAM); monotonic improvement, not guaranteed global optimum.
  \item \textbf{Robustness:} K-medoids less sensitive to outliers than K-means.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
K-means assumes roughly spherical clusters, is sensitive to scaling/initialization, and handles outliers poorly.
K-medoids is more robust but slower on large datasets.
\end{pitfall}

Advantages: K-means is fast; K-medoids is robust.\\
Limitations: K-means sensitive to outliers; K-medoids is slower.

\textbf{Key takeaways:} K-means is fast but sensitive; K-medoids is more robust but costlier.
}

% 3.5 Agglomerative Clustering
\DefineTeach{3.5}{%
\begin{examlikelihood}{High}
Common: list agglomerative steps and compute one merge with a given linkage.
Answer: start with singletons, merge closest by linkage, update distances, repeat.
\end{examlikelihood}

\begin{examfavorite}
Given a small distance matrix, perform one or two agglomerative merges and state the linkage used.
Answer: compute linkage distances and merge the smallest pair step by step.
\end{examfavorite}

Why (motivation): We want a hierarchy of clusters and do not want to pre‑specify $K$.\\
What (definition): Agglomerative hierarchical clustering starts with each point as its own cluster and repeatedly merges the closest clusters.\\
How (procedure/usage): Choose a distance metric + linkage rule, then merge until one cluster remains (cut the dendrogram to pick $K$).

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Algorithm (simplified):} start with $n$ singleton clusters; compute inter‑cluster distances; merge closest pair; update distances; repeat.
  \item \textbf{Single linkage:} $d(A,B)=\min_{x\in A,y\in B} d(x,y)$.
  \item \textbf{Complete linkage:} $d(A,B)=\max_{x\in A,y\in B} d(x,y)$.
  \item \textbf{Average linkage:} $d(A,B)=\frac{1}{|A||B|}\sum_{x\in A}\sum_{y\in B} d(x,y)$.
  \item \textbf{Centroid linkage:} $d(A,B)=\|\mu_A-\mu_B\|$.
  \item \textbf{Ward:} merge that yields the smallest increase in within‑cluster SSE.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Single linkage can “chain” clusters; results depend heavily on scaling and the chosen linkage.
\end{pitfall}

Advantages: produces a full hierarchy; no need to choose $K$ upfront.\\
Limitations: $O(n^2)$ memory/time; early merges are irreversible.

\begin{visualbox}
\centering
\includegraphics[height=0.18\textheight]{asset/dendogram1.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/dendogram2.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/dendogram3.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/dendogram4.png}
\end{visualbox}

\textbf{Key takeaways:} Agglomerative builds a dendrogram using a linkage rule; choose the cut for $K$.
}

% 3.6 DBSCAN
\DefineTeach{3.6}{%
\begin{examlikelihood}{High}
Often: define $\varepsilon$ and MinPts, classify core/border/noise points.
Answer: core has $\ge$ MinPts in $\varepsilon$-neighborhood; border within $\varepsilon$ of a core; noise otherwise.
\end{examlikelihood}

\begin{examfavorite}
Given a small plot, label core/border/noise and explain why DBSCAN finds non‑spherical clusters.
Answer: use density reachability; clusters can follow arbitrary shapes.
\end{examfavorite}

Why (motivation): K‑based methods struggle with arbitrary shapes and noise.\\
What (definition): DBSCAN groups dense regions using $\varepsilon$‑neighborhoods and MinPts; points in sparse regions become noise.\\
How (procedure/usage): Pick $\varepsilon$ and MinPts, mark core points, expand clusters via density reachability, label remaining as noise/border.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Parameters:} $\varepsilon$ (radius), MinPts (minimum neighbors).
  \item \textbf{Core point:} has at least MinPts points in its $\varepsilon$‑neighborhood.
  \item \textbf{Border point:} within $\varepsilon$ of a core but not itself core.
  \item \textbf{Noise:} not reachable from any core.
  \item \textbf{Density‑reachable/connected:} chains of core points link a cluster.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Bad $\varepsilon$/MinPts choices: too small $\rightarrow$ many noise points; too large $\rightarrow$ merged clusters.
\end{pitfall}

\textbf{Advantages:} no need to choose $K$; finds arbitrary‑shaped clusters; handles noise well.\\
\textbf{Limitations:} struggles with varying density; parameter sensitive; distance metrics degrade in high dimensions.

\begin{visualbox}
\centering
\includegraphics[width=0.8\linewidth]{asset/DBSCAN.png}
\vspace{2mm}
\includegraphics[width=0.8\linewidth]{asset/DBSCAN2.png}
\end{visualbox}

\textbf{Key takeaways:} DBSCAN clusters by density and naturally flags noise; choose $\varepsilon$ and MinPts carefully.
}

% 4.1 Introduction (Frequent Itemsets)
\DefineTeach{4.1}{%
\begin{examlikelihood}{High}
Often: define itemset, support, and why frequent itemsets matter.
Answer: itemset is a set of items; support is fraction of transactions; frequent if above minsup.
\end{examlikelihood}

\begin{examfavorite}
Compute support for a small transaction set and identify frequent itemsets above a threshold.
Answer: count transactions containing the itemset, divide by total, compare to minsup.
\end{examfavorite}

Why (motivation): Discover co‑occurrence patterns in transaction data (e.g., market baskets).\\
What (definition): An itemset is a set of items; a frequent itemset appears in at least a minimum fraction of transactions.\\
How (procedure/usage): Count itemset support across transactions and keep those meeting a minimum support threshold.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Transaction DB:} each transaction is a set of items.
  \item \textbf{Itemset:} a subset of items (e.g., $\{A,B\}$).
  \item \textbf{Support:} $\mathrm{supp}(X)=\frac{\#\text{transactions containing }X}{\#\text{transactions}}$.
  \item \textbf{Frequent:} $\mathrm{supp}(X)\ge \text{min\_support}$.
  \item \textbf{Apriori idea:} if an itemset is frequent, all its subsets are frequent.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Mixing up support count (absolute) vs support (fraction); always state which one you use.
\end{pitfall}

Advantages: highlights common co-occurrence patterns.\\
Limitations: can generate many itemsets; sensitive to min support.

\begin{visualbox}
\centering
\includegraphics[height=0.18\textheight]{asset/frequent-itemset.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/frequent-itemset2.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/frequent-itemset3.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/frequent-itemset4.png}
\end{visualbox}

\textbf{Key takeaways:} Frequent itemsets reveal common co‑occurrences; support is the key measure.
}

% 4.2 Properties of Frequent Itemsets
\DefineTeach{4.2}{%
\begin{examlikelihood}{High}
Often: define closed vs maximal itemsets and compare them with frequent itemsets.
Answer: closed has no superset with same support; maximal has no frequent superset.
\end{examlikelihood}

\begin{examfavorite}
Given a small set of frequent itemsets, identify which are closed and which are maximal.
Answer: maximal = no frequent superset; closed = no superset with equal support.
\end{examfavorite}

Why (motivation): Frequent itemsets can be many; closed/maximal summarize them compactly.\\
What (definition): Closed itemsets keep full support information; maximal itemsets keep only the largest frequent ones.\\
How (procedure/usage): Mine frequent itemsets first, then filter for closed or maximal conditions.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Frequent:} $\mathrm{supp}(X)\ge \text{min\_support}$.
  \item \textbf{Closed:} no proper superset of $X$ has the \emph{same} support.
  \item \textbf{Maximal:} no proper superset of $X$ is frequent.
  \item \textbf{Relationship:} Maximal $\subset$ Closed $\subset$ Frequent.
  \item \textbf{Info loss:} Maximal loses support counts; closed preserves supports for all frequent itemsets.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Confusing “closed” with “maximal”: closed allows frequent supersets as long as support drops; maximal allows no frequent supersets.
\end{pitfall}

Advantages: closed/maximal reduce output size.\\
Limitations: maximal loses support info; closed can still be large.

\begin{visualbox}
\centering
\begin{tikzpicture}[
  box/.style={draw, rounded corners, minimum width=36mm, minimum height=7mm, align=center}
]
\node[box] (freq) {Frequent};
\node[box, below=4mm of freq] (closed) {Closed};
\node[box, below=4mm of closed] (max) {Maximal};
\draw[->, thick] (freq) -- (closed);
\draw[->, thick] (closed) -- (max);
\end{tikzpicture}
\end{visualbox}

\textbf{Key takeaways:} Closed keeps support info; maximal is a compact summary but loses support counts.
}

% 4.3 Apriori Algorithm
\DefineTeach{4.3}{%
\begin{examlikelihood}{High}
Common: outline Apriori steps and generate $C_k$ and $L_k$ for a tiny dataset.
Answer: build $L_1$, generate/prune $C_k$, count supports, repeat to get $L_k$.
\end{examlikelihood}

\begin{examfavorite}
Show candidate generation and pruning using the Apriori property.
Answer: join $L_{k-1}$ to form $C_k$, prune any candidate with an infrequent subset.
\end{examfavorite}

Why (motivation): Brute‑force itemset search is exponential; Apriori prunes early.\\
What (definition): Apriori uses the downward‑closure property: all subsets of a frequent itemset are frequent.\\
How (procedure/usage): Iteratively generate candidates and prune using frequent subsets.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Step 1:} find frequent 1‑itemsets $L_1$.
  \item \textbf{Step 2:} generate candidate $k$‑itemsets $C_k$ from $L_{k-1}$ (join step).
  \item \textbf{Step 3:} prune any candidate in $C_k$ with an infrequent $(k-1)$‑subset.
  \item \textbf{Step 4:} scan DB to count supports; keep $L_k$ (frequent candidates).
  \item \textbf{Stop:} when $L_k$ is empty.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Forgetting the prune step (using the Apriori property) leads to too many candidates.
\end{pitfall}

\textbf{Advantages:} strong pruning via downward‑closure; easy to implement and explain.\\
\textbf{Limitations:} challenging candidate generation; each candidate must be tested against the whole dataset.

\begin{visualbox}
\centering
\includegraphics[width=0.78\linewidth]{asset/apriori.png}
\vspace{2mm}
\includegraphics[width=0.78\linewidth]{asset/apriori2.png}
\vspace{2mm}
\includegraphics[width=0.78\linewidth]{asset/apriori3.png}
\end{visualbox}

\textbf{Key takeaways:} Apriori alternates candidate generation and pruning using downward‑closure.
}

% 4.4 FP-Growth Algorithm
\DefineTeach{4.4}{%
\begin{examlikelihood}{High}
Often: explain why FP‑Growth is faster than Apriori and outline its steps.
Answer: no candidate generation; build FP-tree in two passes and mine with DFS.
\end{examlikelihood}

\begin{examfavorite}
State the two DB passes and describe the FP‑tree + conditional pattern base idea.
Answer: pass 1 counts and orders items; pass 2 builds FP-tree; mine conditional bases/trees by DFS.
\end{examfavorite}

Why (motivation): Apriori’s candidate explosion and repeated full scans are costly.\\
What (definition): FP‑Growth mines frequent itemsets by building an FP‑tree and doing DFS on conditional pattern bases (no candidate generation).\\
How (procedure/usage): Make two passes, build the FP‑tree, then recursively mine conditional FP‑trees (depth‑first).

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Pass 1:} count item supports; keep frequent items and order them.
  \item \textbf{Pass 2:} build FP‑tree by inserting ordered transactions.
  \item \textbf{DFS mining:} for each item, build its conditional pattern base and conditional FP‑tree; recurse.
  \item \textbf{Output:} frequent itemsets without candidate generation.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Not ordering items by global frequency before building the FP‑tree (tree becomes large and inefficient).
\end{pitfall}

\textbf{Advantages:} avoids candidate generation; only two DB passes; efficient for dense datasets.\\
\textbf{Limitations:} FP‑tree can be large in very sparse data; more complex to implement.

\begin{visualbox}
\centering
\includegraphics[height=0.18\textheight]{asset/fp-tree0.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/fp-tree1.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/fp-tree2.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/fp-tree3.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/fp-tree4.png}
\end{visualbox}

\textbf{Key takeaways:} FP‑Growth uses an FP‑tree and DFS to mine patterns with only two passes.
}

% 5.1 Introduction (Association Rules)
\DefineTeach{5.1}{%
\begin{examlikelihood}{High}
Often: define an association rule and its parts (antecedent/consequent) and relate rules to frequent itemsets.
Answer: rule $X \rightarrow Y$ with $X \cap Y = \emptyset$; rules are generated from frequent itemsets.
\end{examlikelihood}

\begin{examfavorite}
Given a tiny basket dataset, propose a reasonable rule and interpret it in words.
Answer: e.g., $\{\text{bread}\} \rightarrow \{\text{butter}\}$ means baskets with bread often also contain butter.
\end{examfavorite}

Why (motivation): We want simple, actionable “if‑then” patterns that summarize co‑purchase behavior.\\
What (definition): An association rule is an implication $X \rightarrow Y$ between disjoint itemsets; $X$ is the antecedent, $Y$ the consequent.\\
How (procedure/usage): Mine frequent itemsets first, then generate candidate rules and keep those that pass quality thresholds.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Rule form:} $X \rightarrow Y$, with $X \cap Y = \emptyset$.
  \item \textbf{Antecedent:} left side (condition); \textbf{Consequent:} right side (outcome).
  \item \textbf{From itemsets:} rules are derived from frequent itemsets of size $\ge 2$.
  \item \textbf{Interpretation:} “If $X$ occurs in a transaction, $Y$ tends to occur too.”
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Treating rules as causal (“$X$ causes $Y$”). Association rules are correlational and can be misleading without lift/base‑rate checks.
\end{pitfall}

\textbf{Advantages:} highly interpretable patterns; useful for merchandising, cross‑sell, and recommendation hints.\\
\textbf{Limitations:} many trivial/spurious rules; sensitive to thresholds; does not imply causation.

\begin{visualbox}
\centering
\includegraphics[height=0.18\textheight]{asset/assoc-rules.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/assoc-rules2.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/assoc-rules3.png}
\end{visualbox}

\textbf{Key takeaways:}
\begin{itemize}
  \item Association rules express “if‑then” co‑occurrence patterns.
  \item Rules come from frequent itemsets and need quality thresholds to be useful.
\end{itemize}
}

% 5.2 Generating Association Rules
\DefineTeach{5.2}{%
\begin{examlikelihood}{High}
Often: describe how to generate rules from a frequent itemset and how to prune them.
Answer: split a frequent itemset $I$ into $X$ and $Y=I\setminus X$; keep rules with confidence above min\_conf (and usually lift).
\end{examlikelihood}

\begin{examfavorite}
Given a frequent itemset $I=\{A,B,C\}$, list candidate rules and compute confidence for one.
Answer: all non‑empty proper splits, e.g., $AB\rightarrow C$, $A\rightarrow BC$, etc.
\end{examfavorite}

Why (motivation): Mining frequent itemsets is not enough; we need directional rules that are strong and useful.\\
What (definition): A rule $X \rightarrow Y$ is generated from a frequent itemset $I$ by choosing $X \subset I$ and $Y=I\setminus X$.\\
How (procedure/usage): For each frequent itemset, enumerate non‑empty splits, compute confidence (and lift), and prune by thresholds.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Candidates:} for itemset $I$, all rules $X \rightarrow I\setminus X$ with $\emptyset \subset X \subset I$.
  \item \textbf{Confidence:} $\mathrm{conf}(X\rightarrow Y)=\frac{\mathrm{supp}(X\cup Y)}{\mathrm{supp}(X)}$.
  \item \textbf{Pruning:} if a rule fails min\_conf, any rule with smaller antecedent may still pass (use careful pruning).
  \item \textbf{Typical flow:} mine frequent itemsets $\rightarrow$ generate rules $\rightarrow$ filter by min\_conf (and lift).
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Generating rules from infrequent itemsets or forgetting to enforce $X \cap Y = \emptyset$.
\end{pitfall}

\textbf{Advantages:} systematic generation from frequent itemsets; simple to explain and compute.\\
\textbf{Limitations:} rule count can explode; high confidence can be misleading without lift.

\begin{visualbox}
\centering
\begin{tikzpicture}[
  box/.style={draw, rounded corners, minimum height=7mm, minimum width=24mm, align=center},
  arrow/.style={-Latex, thick},
  node distance=10mm and 14mm
]
\node[box] (fi) {Frequent\\itemset $I$};
\node[box, right=of fi] (split) {Split $I$\\into $X,Y$};
\node[box, right=of split] (rule) {Rules\\$X\rightarrow Y$};
\draw[arrow] (fi) -- (split);
\draw[arrow] (split) -- (rule);
\end{tikzpicture}
\end{visualbox}

\textbf{Key takeaways:}
\begin{itemize}
  \item Rules come from splitting frequent itemsets into antecedent and consequent.
  \item Use confidence (and lift) thresholds to prune weak or misleading rules.
\end{itemize}
}

% 5.3 Evaluation (support, confidence, lift, conviction)
\DefineTeach{5.3}{%
\begin{examlikelihood}{High}
Often: compute support, confidence, and lift for a rule and interpret the result.
Answer: use counts (or a confusion matrix) to compute $P(X \cap Y)$, $P(Y|X)$, and $P(Y|X)/P(Y)$.
\end{examlikelihood}

\begin{examfavorite}
Interpret lift values: 1, much greater than 1, and much less than 1.
Answer: 1 means independence; $\gg 1$ strong positive association; $\ll 1$ negative association.
\end{examfavorite}

Why (motivation): There are many rules; we need measures that balance coverage, reliability, and base rates.\\
What (definition): Support measures how often the rule occurs, confidence measures how often it is correct when it fires, and lift adjusts confidence by the base rate of $Y$ (independence check).\\
How (procedure/usage): Compute metrics from counts (or confusion matrix), then filter by minsup/minconf and prefer high lift (and reasonable support).

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Support:} $\mathrm{supp}(X\rightarrow Y)=\mathrm{supp}(X\cup Y)=P(X \cap Y)$.
  \item \textbf{Confidence:} $\mathrm{conf}(X\rightarrow Y)=\frac{P(X \cap Y)}{P(X)}=P(Y|X)$.
  \item \textbf{Lift:} $\mathrm{lift}(X\rightarrow Y)=\frac{P(Y|X)}{P(Y)}=\frac{P(X \cap Y)}{P(X)P(Y)}$.
  \item \textbf{Conviction:} $\mathrm{conv}(X\rightarrow Y)=\frac{1-P(Y)}{1-\mathrm{conf}}$ (directional strength).
  \item \textbf{Confusion matrix view:} predict $Y$ when $X$ occurs.
  \item \textbf{TP:} $X$ and $Y$; \textbf{FP:} $X$ and not $Y$; \textbf{FN:} $Y$ and not $X$; \textbf{TN:} neither.
  \item \textbf{Mapping:} confidence = precision = $\frac{TP}{TP+FP}$; support = $\frac{TP}{N}$.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
High confidence can be trivial if $Y$ is very common; always check lift (and support).
\end{pitfall}

\textbf{Advantages:} simple, interpretable rule quality metrics; lift corrects for base rates.\\
\textbf{Limitations:} can favor rare items or tiny supports; does not imply causation.

\begin{visualbox}
\centering
\includegraphics[height=0.18\textheight]{asset/conf-matrix1.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/conf-matrix2.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/conf-matrix3.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/conf-matrix4.png}
\end{visualbox}

\textbf{Key takeaways:}
\begin{itemize}
  \item Lift = 1 means independence; $\gg 1$ strong positive association; $\ll 1$ negative association.
  \item Use support + confidence + lift together to judge rule quality.
\end{itemize}
}

% 5.5 Simpson's Paradox
\DefineTeach{5.5}{%
\begin{examlikelihood}{High}
Often: define Simpson's paradox and explain how it can affect association rules or metrics.
Answer: a trend in aggregated data reverses after stratifying by a confounder; subgroup analysis is required.
\end{examlikelihood}

\begin{examfavorite}
Given a table with two subgroups, identify the paradox and name the confounder.
Answer: the confounder changes the base rates; the combined trend is misleading.
\end{examfavorite}

Why (motivation): Aggregated metrics can hide or reverse real relationships, leading to bad rules or decisions.\\
What (definition): Simpson's paradox occurs when an association in pooled data reverses after conditioning on a confounding variable (a third variable).\\
How (procedure/usage): Always check key subgroups (stratify), compare within‑group metrics, and look for confounders.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Paradox signal:} pooled trend $\ne$ subgroup trends (often reversed).
  \item \textbf{Cause:} confounding variable with different base rates across groups.
  \item \textbf{Fix:} stratify by the confounder; compare like with like.
  \item \textbf{Rule mining:} a high‑lift rule may disappear or flip after stratification.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Reporting only aggregated confidence/lift and ignoring confounding variables that drive subgroup differences.
\end{pitfall}

\textbf{Advantages:} forces careful interpretation; highlights need for subgroup analysis.\\
\textbf{Limitations:} requires enough data per subgroup; choosing stratification variables can be non‑obvious.

\begin{visualbox}
\centering
\includegraphics[height=0.2\textheight]{asset/simpson.png}
\vspace{1mm}
\includegraphics[height=0.2\textheight]{asset/simpson2.png}
\vspace{1mm}
\includegraphics[height=0.2\textheight]{asset/confounding.png}
\end{visualbox}

\textbf{Key takeaways:}
\begin{itemize}
  \item Aggregation can reverse associations; always check subgroups.
  \item Confounders drive the paradox; stratify before trusting a rule.
\end{itemize}
}

% 6.1 Temporal Data
\DefineTeach{6.1}{%
\begin{examlikelihood}{High}
Often: define temporal data and list its key characteristics (order, granularity, seasonality).
Answer: temporal data is indexed by time; ordering matters; it may show trend/seasonality and irregular sampling.
\end{examlikelihood}

\begin{examfavorite}
Explain why random train/test splits are wrong for time series.
Answer: time order must be preserved to avoid leakage; use forward‑chaining or hold‑out of later periods.
\end{examfavorite}

Why (motivation): Time‑indexed data appears in finance, sensors, logs, and business KPIs; ignoring time breaks analysis.\\
What (definition): Temporal data is a sequence of observations indexed by time; the order and spacing carry meaning.\\
How (procedure/usage): Inspect timestamps, granularity, missing/irregular intervals, and basic patterns (trend/seasonality) before modeling.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Time index:} timestamp or ordered time steps.
  \item \textbf{Granularity:} sampling rate (hourly/daily/etc.).
  \item \textbf{Patterns:} trend, seasonality, cycles, noise.
  \item \textbf{Irregularity:} gaps, unequal spacing, missing data.
  \item \textbf{Leakage risk:} never use future data to predict the past.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Treating temporal data as i.i.d. and shuffling it; this destroys order and causes leakage.
\end{pitfall}

\textbf{Advantages:} captures dynamics and change over time; supports forecasting and anomaly detection.\\
\textbf{Limitations:} non‑stationarity and irregular sampling complicate modeling; needs careful preprocessing.

\begin{visualbox}
\centering
\end{visualbox}

\textbf{Key takeaways:}
\begin{itemize}
  \item Temporal data has ordered observations; order is part of the signal.
  \item Always respect time when splitting data and interpreting patterns.
\end{itemize}
}

% 6.2 Introduction to Time Series
\DefineTeach{6.2}{%
\begin{examlikelihood}{High}
Often: define a time series and its components (trend, seasonality, noise).
Answer: a time series is an ordered sequence of observations; components include trend/seasonal/cycle/noise.
\end{examlikelihood}

\begin{examfavorite}
Identify components from a small plot and state if the series is stationary.
Answer: describe trend/seasonality; stationary means constant mean/variance over time.
\end{examfavorite}

Why (motivation): Time series models use structure in time to forecast and detect anomalies.\\
What (definition): A time series is a sequence $\{x_t\}$ indexed by time; it can be decomposed into components.\\
How (procedure/usage): Visualize, decompose, and check stationarity before choosing models.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{Components:} trend, seasonality, cycle, noise.
  \item \textbf{Stationarity:} mean/variance (and autocovariance) stable over time.
  \item \textbf{Decomposition:} additive $x_t=T_t+S_t+C_t+e_t$ or multiplicative $x_t=T_t\cdot S_t\cdot C_t\cdot e_t$.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Ignoring seasonality or trend and fitting a stationary model to a non‑stationary series.
\end{pitfall}

\textbf{Advantages:} explicit modeling of temporal structure; enables forecasting.\\
\textbf{Limitations:} non‑stationarity and seasonality can break simple models.

\begin{visualbox}
\centering
\includegraphics[height=0.2\textheight]{asset/time-series1.png}
\vspace{1mm}
\includegraphics[height=0.2\textheight]{asset/time-series2.png}
\end{visualbox}

\textbf{Key takeaways:}
\begin{itemize}
  \item Time series have components; visualize and decompose first.
  \item Stationarity matters for many models.
\end{itemize}
}

% 6.3 Analysis
\DefineTeach{6.3}{%
\begin{examlikelihood}{High}
Often: explain ACF/PACF or identify lags and autocorrelation from a plot.
Answer: ACF shows correlation with past lags; PACF shows direct lag effects.
\end{examlikelihood}

\begin{examfavorite}
Given ACF/PACF plots, suggest a simple AR or MA order.
Answer: cut‑off in ACF suggests MA(q); cut‑off in PACF suggests AR(p).
\end{examfavorite}

Why (motivation): We need to understand temporal dependence before choosing a forecasting model.\\
What (definition): Time‑series analysis studies autocorrelation and lag structure; \textbf{ACF} (Autocorrelation Function) measures correlation of a series with its past lags. \textbf{White noise} is a series with zero mean, constant variance, and no autocorrelation at non‑zero lags.\\
How (procedure/usage): Plot the series, check ACF, and inspect residuals for remaining structure.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{ACF:} correlation of $x_t$ with $x_{t-k}$ across lags.
  \item \textbf{Autocorrelation formula:} $\rho_k=\frac{\sum_{t=k+1}^{n}(x_t-\bar{x})(x_{t-k}-\bar{x})}{\sum_{t=1}^{n}(x_t-\bar{x})^2}$.
  \item \textbf{White noise:} $\rho_k \approx 0$ for all $k>0$.
  \item \textbf{Rule of thumb:} ACF cut‑off suggests MA order.
  \item \textbf{Diagnostics:} residuals should look like white noise.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Choosing model orders from a single noisy plot; verify with diagnostics and hold‑out tests.
\end{pitfall}

\textbf{Advantages:} reveals dependence structure; guides model selection.\\
\textbf{Limitations:} plots can be noisy; patterns can be ambiguous.

\begin{visualbox}
\centering
\includegraphics[height=0.18\textheight]{asset/time-analysis.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/time-analysis2.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/time-analysis3.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/moving-average.png}
\end{visualbox}

\textbf{Key takeaways:}
\begin{itemize}
  \item Use ACF/PACF to understand lag structure and pick model orders.
  \item Always validate with residual checks and time‑based splits.
\end{itemize}
}

% 6.4 Forecasting
\DefineTeach{6.4}{%
\begin{examlikelihood}{High}
Often: distinguish AR, MA, ARMA, and ARIMA; explain when differencing is needed.
Answer: AR uses past values, MA uses past errors, ARMA combines both, ARIMA adds differencing for non‑stationary series.
\end{examlikelihood}

\begin{examfavorite}
Given a short series, explain how to split data and evaluate forecasts.
Answer: keep chronological order; train on earlier data and test on later data; report MAE/MSE/RMSE or MAPE.
\end{examfavorite}

Why (motivation): We want models that capture temporal dependence and produce reliable forecasts.\\
What (definition): \textbf{AR} (Autoregressive) models regress on past values, \textbf{MA} (Moving Average) models regress on past errors, \textbf{ARMA} (Autoregressive Moving Average) combines AR+MA, and \textbf{ARIMA} (Autoregressive Integrated Moving Average) adds differencing to handle non‑stationarity.\\
How (procedure/usage): Check stationarity, difference if needed, choose AR/MA orders, fit, and evaluate on a time‑ordered test set.

\begin{cheatsheet}
\begin{itemize}
  \item \textbf{AR($p$):} $x_t=c+\sum_{i=1}^{p}\phi_i x_{t-i}+\varepsilon_t$.
  \item \textbf{MA($q$):} $x_t=c+\sum_{i=1}^{q}\theta_i \varepsilon_{t-i}+\varepsilon_t$.
  \item \textbf{ARMA($p,q$):} combine AR and MA terms.
  \item \textbf{ARIMA($p,d,q$):} difference $d$ times to make series stationary, then ARMA.
  \item \textbf{Stationarity:} constant mean/variance; use differencing to remove trend.
  \item \textbf{Evaluation:} MAE, MSE/RMSE, MAPE; time‑ordered train/test split.
  \item \textbf{Granger causality:} $X$ helps predict $Y$ if past $X$ improves forecast of $Y$ beyond past $Y$ alone.
\end{itemize}
\end{cheatsheet}

\begin{pitfall}
Randomly shuffling time series for train/test or claiming “causality” from Granger tests; it is predictive, not causal in a real‑world sense.
\end{pitfall}

\textbf{Advantages:} interpretable baselines; fast to fit; good for short‑term forecasting.\\
\textbf{Limitations:} assumes linear structure; sensitive to non‑stationarity and outliers; limited for complex patterns.

\begin{visualbox}
\centering
\includegraphics[height=0.18\textheight]{asset/6.4.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/6.4-2.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/6.4-3.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/6.4-4.png}
\vspace{1mm}
\includegraphics[height=0.18\textheight]{asset/6.4-5.png}
\end{visualbox}

\textbf{Key takeaways:}
\begin{itemize}
  \item AR/MA/ARMA model dependence; ARIMA handles non‑stationary series via differencing.
  \item Always evaluate with time‑ordered splits and forecast error metrics.
\end{itemize}
}
